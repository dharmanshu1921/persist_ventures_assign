{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:56:10,525 - INFO: Starting data fetch from: https://api.socialverseapp.com/posts/view\n",
      "2024-12-12 14:56:11,546 - INFO: Page 1: Retrieved 1000 items\n",
      "2024-12-12 14:56:12,558 - INFO: Page 2: Retrieved 1000 items\n",
      "2024-12-12 14:56:14,705 - INFO: Page 3: Retrieved 1000 items\n",
      "2024-12-12 14:56:16,369 - INFO: Page 4: Retrieved 1000 items\n",
      "2024-12-12 14:56:18,906 - INFO: Page 5: Retrieved 1000 items\n",
      "2024-12-12 14:56:19,992 - INFO: Page 6: Retrieved 1000 items\n",
      "2024-12-12 14:56:24,599 - INFO: Page 7: Retrieved 949 items\n",
      "2024-12-12 14:56:24,637 - INFO: JSON data stored at: api_logs/json/view.json\n",
      "2024-12-12 14:56:24,647 - INFO: CSV data stored at: api_logs/csv/view.csv\n",
      "2024-12-12 14:56:24,648 - INFO: Completed data fetch. Total items retrieved: 6949\n",
      "2024-12-12 14:56:24,649 - INFO: Starting data fetch from: https://api.socialverseapp.com/posts/like\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 6949 view items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:56:25,461 - INFO: Page 1: Retrieved 1000 items\n",
      "2024-12-12 14:56:26,634 - INFO: Page 2: Retrieved 279 items\n",
      "2024-12-12 14:56:26,640 - INFO: JSON data stored at: api_logs/json/like.json\n",
      "2024-12-12 14:56:26,642 - INFO: CSV data stored at: api_logs/csv/like.csv\n",
      "2024-12-12 14:56:26,643 - INFO: Completed data fetch. Total items retrieved: 1279\n",
      "2024-12-12 14:56:26,643 - INFO: Starting data fetch from: https://api.socialverseapp.com/posts/inspire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1279 like items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:56:28,786 - INFO: Page 1: Retrieved 268 items\n",
      "2024-12-12 14:56:28,788 - INFO: JSON data stored at: api_logs/json/inspire.json\n",
      "2024-12-12 14:56:28,789 - INFO: CSV data stored at: api_logs/csv/inspire.csv\n",
      "2024-12-12 14:56:28,789 - INFO: Completed data fetch. Total items retrieved: 268\n",
      "2024-12-12 14:56:28,790 - INFO: Starting data fetch from: https://api.socialverseapp.com/posts/rating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 268 inspire items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:56:30,081 - INFO: Page 1: Retrieved 1000 items\n",
      "2024-12-12 14:56:32,581 - INFO: Page 2: Retrieved 1000 items\n",
      "2024-12-12 14:56:33,773 - INFO: Page 3: Retrieved 907 items\n",
      "2024-12-12 14:56:33,802 - INFO: JSON data stored at: api_logs/json/rate.json\n",
      "2024-12-12 14:56:33,811 - INFO: CSV data stored at: api_logs/csv/rate.csv\n",
      "2024-12-12 14:56:33,812 - INFO: Completed data fetch. Total items retrieved: 2907\n",
      "2024-12-12 14:56:33,812 - INFO: Starting data fetch from: https://api.socialverseapp.com/posts/summary/get\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 2907 rate items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:56:37,150 - INFO: Page 1: Retrieved 1000 items\n",
      "2024-12-12 14:56:40,125 - INFO: Page 2: Retrieved 857 items\n",
      "2024-12-12 14:56:40,441 - INFO: JSON data stored at: api_logs/json/summary.json\n",
      "2024-12-12 14:56:40,710 - INFO: CSV data stored at: api_logs/csv/summary.csv\n",
      "2024-12-12 14:56:40,711 - INFO: Completed data fetch. Total items retrieved: 1857\n",
      "2024-12-12 14:56:40,711 - INFO: Starting data fetch from: https://api.socialverseapp.com/users/get_all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1857 summary items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:56:42,649 - INFO: Page 1: Retrieved 1000 items\n",
      "2024-12-12 14:56:44,421 - INFO: Page 2: Retrieved 306 items\n",
      "2024-12-12 14:56:44,464 - INFO: JSON data stored at: api_logs/json/user.json\n",
      "2024-12-12 14:56:44,476 - INFO: CSV data stored at: api_logs/csv/user.csv\n",
      "2024-12-12 14:56:44,477 - INFO: Completed data fetch. Total items retrieved: 1306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1306 user items.\n",
      "\n",
      "Data Lengths:\n",
      "view: 6949 items\n",
      "like: 1279 items\n",
      "inspire: 268 items\n",
      "rate: 2907 items\n",
      "summary: 1857 items\n",
      "user: 1306 items\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Union, Tuple, Any\n",
    "from enum import Enum\n",
    "\n",
    "class DataType(Enum):\n",
    "    \"\"\"Enum for different types of data being fetched\"\"\"\n",
    "    VIEW = \"view\"\n",
    "    LIKE = \"like\"\n",
    "    INSPIRE = \"inspire\"\n",
    "    RATE = \"rate\"\n",
    "    SUMMARY = \"summary\"\n",
    "    USER = \"user\"\n",
    "\n",
    "    @property\n",
    "    def is_paginated(self) -> bool:\n",
    "        \"\"\"Determine if the endpoint supports pagination\"\"\"\n",
    "        return True  \n",
    "\n",
    "class APIConfig:\n",
    "    \"\"\"Configuration class for API endpoints and parameters\"\"\"\n",
    "    BASE_URL = \"https://api.socialverseapp.com\"\n",
    "    RESONANCE_ALGORITHM = \"resonance_algorithm_cjsvervb7dbhss8bdrj89s44jfjdbsjd0xnjkbvuire8zcjwerui3njfbvsujc5if\"\n",
    "    FLIC_TOKEN = \"flic_6e2d8d25dc29a4ddd382c2383a903cf4a688d1a117f6eb43b35a1e7fadbb84b8\"\n",
    "    PAGE_SIZE = 1000\n",
    "\n",
    "    @classmethod\n",
    "    def get_endpoint(cls, data_type: DataType) -> Tuple[str, Dict, Dict]:\n",
    "        \"\"\"Get API endpoint and parameters for a given data type\"\"\"\n",
    "        base_params = {\"page\": 1, \"page_size\": cls.PAGE_SIZE}\n",
    "        base_headers = {}\n",
    "        \n",
    "        endpoints = {\n",
    "            DataType.VIEW: (\"/posts/view\", {\"resonance_algorithm\": cls.RESONANCE_ALGORITHM}, {}),\n",
    "            DataType.LIKE: (\"/posts/like\", {\"resonance_algorithm\": cls.RESONANCE_ALGORITHM}, {}),\n",
    "            DataType.INSPIRE: (\"/posts/inspire\", {\"resonance_algorithm\": cls.RESONANCE_ALGORITHM}, {}),\n",
    "            DataType.RATE: (\"/posts/rating\", {\"resonance_algorithm\": cls.RESONANCE_ALGORITHM}, {}),\n",
    "            DataType.SUMMARY: (\"/posts/summary/get\", {}, {\"Flic-Token\": cls.FLIC_TOKEN}),\n",
    "            DataType.USER: (\"/users/get_all\", {}, {\"Flic-Token\": cls.FLIC_TOKEN})\n",
    "        }\n",
    "        \n",
    "        endpoint, extra_params, extra_headers = endpoints[data_type]\n",
    "        params = {**base_params, **extra_params}\n",
    "        headers = {**base_headers, **extra_headers}\n",
    "        \n",
    "        return f\"{cls.BASE_URL}{endpoint}\", params, headers\n",
    "\n",
    "class APIDataFetcher:\n",
    "    \"\"\"A comprehensive utility for fetching paginated data from APIs with advanced features.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_log_dir: str = 'api_logs'):\n",
    "        \"\"\"Initialize the APIDataFetcher with logging and storage configurations.\"\"\"\n",
    "        self.base_log_dir = base_log_dir\n",
    "        self._setup_directories()\n",
    "        self._setup_logging()\n",
    "    \n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Set up necessary directories for logs and data storage\"\"\"\n",
    "        os.makedirs(self.base_log_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.base_log_dir, 'json'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.base_log_dir, 'csv'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.base_log_dir, 'logs'), exist_ok=True)\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure logging with both file and console handlers\"\"\"\n",
    "        log_file = os.path.join(self.base_log_dir, 'logs', f'api_fetch_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def fetch_data(\n",
    "        self,\n",
    "        data_type: DataType,\n",
    "        export_csv: bool = True\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch data from an API with comprehensive error handling and logging.\n",
    "        \n",
    "        :param data_type: Type of data to fetch (from DataType enum)\n",
    "        :param export_csv: Whether to export data to CSV\n",
    "        :return: List of data items\n",
    "        \"\"\"\n",
    "        api_url, params, headers = APIConfig.get_endpoint(data_type)\n",
    "        \n",
    "        self.logger.info(f\"Starting data fetch from: {api_url}\")\n",
    "        \n",
    "        try:\n",
    "            data = self._fetch_paginated_data(api_url, params, headers, data_type)\n",
    "            \n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            json_filename = f\"{data_type.value}.json\"\n",
    "            csv_filename = f\"{data_type.value}.csv\"\n",
    "            \n",
    "            json_path = os.path.join(self.base_log_dir, 'json', json_filename)\n",
    "            csv_path = os.path.join(self.base_log_dir, 'csv', csv_filename)\n",
    "            \n",
    "\n",
    "            os.makedirs(os.path.join(self.base_log_dir, 'json'), exist_ok=True)\n",
    "            os.makedirs(os.path.join(self.base_log_dir, 'csv'), exist_ok=True)\n",
    "            \n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            self.logger.info(f\"JSON data stored at: {json_path}\")\n",
    "            \n",
    "            if export_csv and data:\n",
    "                if data: \n",
    "                    try:\n",
    "                        fieldnames = list(data[0].keys())\n",
    "                        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                            writer.writeheader()\n",
    "                            writer.writerows(data)\n",
    "                        self.logger.info(f\"CSV data stored at: {csv_path}\")\n",
    "                    except Exception as csv_error:\n",
    "                        self.logger.error(f\"Error writing CSV: {csv_error}\")\n",
    "            \n",
    "            self.logger.info(f\"Completed data fetch. Total items retrieved: {len(data)}\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during data fetch: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _fetch_paginated_data(\n",
    "        self,\n",
    "        api_url: str,\n",
    "        params: Dict,\n",
    "        headers: Dict,\n",
    "        data_type: DataType\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Fetch data from paginated endpoints\"\"\"\n",
    "        all_data = []\n",
    "        page = 1\n",
    "        \n",
    "        while True:\n",
    "            params['page'] = page\n",
    "            response = self._make_api_request(api_url, params, headers)\n",
    "            \n",
    "            if data_type in [DataType.VIEW, DataType.LIKE, DataType.INSPIRE, DataType.RATE]:\n",
    "                page_data = response.get('posts', [])\n",
    "            elif data_type == DataType.SUMMARY:\n",
    "                page_data = response.get('posts', [])\n",
    "            elif data_type == DataType.USER:\n",
    "                page_data = response.get('users', [])\n",
    "            else:\n",
    "                page_data = []\n",
    "            \n",
    "            if not page_data:\n",
    "                break\n",
    "                \n",
    "            all_data.extend(page_data)\n",
    "            self.logger.info(f\"Page {page}: Retrieved {len(page_data)} items\")\n",
    "            \n",
    "            if len(page_data) < APIConfig.PAGE_SIZE:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def _make_api_request(self, url: str, params: Dict, headers: Dict) -> Dict:\n",
    "        \"\"\"Make API request with error handling\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"API Request Error: {e}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise Exception(f\"JSON Parsing Error: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the APIDataFetcher\"\"\"\n",
    "    fetcher = APIDataFetcher()\n",
    "\n",
    "    all_data = {}\n",
    "    \n",
    "    for data_type in DataType:\n",
    "        data = fetcher.fetch_data(data_type)\n",
    "        all_data[data_type.value] = data\n",
    "        \n",
    "        print(f\"Retrieved {len(data)} {data_type.value} items.\")\n",
    "    \n",
    "    print(\"\\nData Lengths:\")\n",
    "    for data_type, data in all_data.items():\n",
    "        if isinstance(data, list):\n",
    "            print(f\"{data_type}: {len(data)} items\")\n",
    "        else:\n",
    "            print(f\"{data_type}: 1 item\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
